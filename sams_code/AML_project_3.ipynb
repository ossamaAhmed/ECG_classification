{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "import pandas as pd\n",
    "train_input = pd.read_csv(\"data/X_train.csv\")\n",
    "train_output = pd.read_csv(\"data/y_train.csv\")\n",
    "test_input = pd.read_csv(\"data/X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16382\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "num_of_train_samples = len(train_input)\n",
    "num_of_features = len(train_input.loc[0]) - 1 # first one is for id\n",
    "print( num_of_features)\n",
    "print( num_of_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id          x0          x1          x2          x3          x4  \\\n",
      "count  10.00000   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
      "mean    4.50000   39.600000   39.400000   38.600000   37.800000   35.500000   \n",
      "std     3.02765  259.779051  263.550628  266.452914  270.179693  273.307417   \n",
      "min     0.00000 -454.000000 -443.000000 -426.000000 -411.000000 -396.000000   \n",
      "25%     2.25000  -61.250000  -96.000000 -131.250000 -164.250000 -184.000000   \n",
      "50%     4.50000   10.500000   16.000000   21.000000   28.000000   37.500000   \n",
      "75%     6.75000  220.500000  257.000000  280.250000  298.750000  282.750000   \n",
      "max     9.00000  394.000000  380.000000  369.000000  358.000000  378.000000   \n",
      "\n",
      "               x5          x6          x7          x8   ...    x16372  x16373  \\\n",
      "count   10.000000   10.000000   10.000000   10.000000   ...       1.0     1.0   \n",
      "mean    33.800000   34.200000   35.700000   36.900000   ...     -47.0   -47.0   \n",
      "std    271.379521  268.516004  265.686135  263.305674   ...       NaN     NaN   \n",
      "min   -374.000000 -346.000000 -321.000000 -317.000000   ...     -47.0   -47.0   \n",
      "25%   -185.750000 -188.250000 -189.750000 -191.750000   ...     -47.0   -47.0   \n",
      "50%     45.000000   47.500000   50.000000   52.000000   ...     -47.0   -47.0   \n",
      "75%    261.000000  252.250000  247.750000  244.250000   ...     -47.0   -47.0   \n",
      "max    387.000000  391.000000  393.000000  396.000000   ...     -47.0   -47.0   \n",
      "\n",
      "       x16374  x16375  x16376  x16377  x16378  x16379  x16380  x16381  \n",
      "count     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0  \n",
      "mean    -46.0   -46.0   -46.0   -46.0   -47.0   -48.0   -49.0   -49.0  \n",
      "std       NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "min     -46.0   -46.0   -46.0   -46.0   -47.0   -48.0   -49.0   -49.0  \n",
      "25%     -46.0   -46.0   -46.0   -46.0   -47.0   -48.0   -49.0   -49.0  \n",
      "50%     -46.0   -46.0   -46.0   -46.0   -47.0   -48.0   -49.0   -49.0  \n",
      "75%     -46.0   -46.0   -46.0   -46.0   -47.0   -48.0   -49.0   -49.0  \n",
      "max     -46.0   -46.0   -46.0   -46.0   -47.0   -48.0   -49.0   -49.0  \n",
      "\n",
      "[8 rows x 16383 columns]\n",
      "             id          y\n",
      "count  10.00000  10.000000\n",
      "mean    4.50000   0.700000\n",
      "std     3.02765   0.948683\n",
      "min     0.00000   0.000000\n",
      "25%     2.25000   0.000000\n",
      "50%     4.50000   0.000000\n",
      "75%     6.75000   1.750000\n",
      "max     9.00000   2.000000\n"
     ]
    }
   ],
   "source": [
    "print(train_input.describe())\n",
    "print(train_output.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>...</th>\n",
       "      <th>x16372</th>\n",
       "      <th>x16373</th>\n",
       "      <th>x16374</th>\n",
       "      <th>x16375</th>\n",
       "      <th>x16376</th>\n",
       "      <th>x16377</th>\n",
       "      <th>x16378</th>\n",
       "      <th>x16379</th>\n",
       "      <th>x16380</th>\n",
       "      <th>x16381</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>357</td>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "      <td>355</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>394</td>\n",
       "      <td>380</td>\n",
       "      <td>369</td>\n",
       "      <td>357</td>\n",
       "      <td>336</td>\n",
       "      <td>308</td>\n",
       "      <td>297</td>\n",
       "      <td>292</td>\n",
       "      <td>288</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>-49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>66</td>\n",
       "      <td>83</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-77</td>\n",
       "      <td>-123</td>\n",
       "      <td>-170</td>\n",
       "      <td>-214</td>\n",
       "      <td>-261</td>\n",
       "      <td>-289</td>\n",
       "      <td>-304</td>\n",
       "      <td>-311</td>\n",
       "      <td>-317</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-217</td>\n",
       "      <td>-225</td>\n",
       "      <td>-231</td>\n",
       "      <td>-236</td>\n",
       "      <td>-240</td>\n",
       "      <td>-242</td>\n",
       "      <td>-245</td>\n",
       "      <td>-247</td>\n",
       "      <td>-249</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>138</td>\n",
       "      <td>134</td>\n",
       "      <td>131</td>\n",
       "      <td>127</td>\n",
       "      <td>123</td>\n",
       "      <td>120</td>\n",
       "      <td>118</td>\n",
       "      <td>115</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-14</td>\n",
       "      <td>-15</td>\n",
       "      <td>-15</td>\n",
       "      <td>-15</td>\n",
       "      <td>-16</td>\n",
       "      <td>-17</td>\n",
       "      <td>-18</td>\n",
       "      <td>-18</td>\n",
       "      <td>-20</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>248</td>\n",
       "      <td>298</td>\n",
       "      <td>330</td>\n",
       "      <td>358</td>\n",
       "      <td>378</td>\n",
       "      <td>387</td>\n",
       "      <td>391</td>\n",
       "      <td>393</td>\n",
       "      <td>396</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-454</td>\n",
       "      <td>-443</td>\n",
       "      <td>-426</td>\n",
       "      <td>-411</td>\n",
       "      <td>-396</td>\n",
       "      <td>-374</td>\n",
       "      <td>-346</td>\n",
       "      <td>-321</td>\n",
       "      <td>-298</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 16383 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   x0   x1   x2   x3   x4   x5   x6   x7   x8   ...    x16372  x16373  \\\n",
       "0   0  357  356  356  356  356  355  354  354  352   ...       NaN     NaN   \n",
       "1   1  394  380  369  357  336  308  297  292  288   ...       NaN     NaN   \n",
       "2   2   22   19   15   11    9    7    7    7    7   ...     -47.0   -47.0   \n",
       "3   3   -1   13   27   45   66   83   88   93   97   ...       NaN     NaN   \n",
       "4   4  -77 -123 -170 -214 -261 -289 -304 -311 -317   ...       NaN     NaN   \n",
       "5   5 -217 -225 -231 -236 -240 -242 -245 -247 -249   ...       NaN     NaN   \n",
       "6   6  138  134  131  127  123  120  118  115  113   ...       NaN     NaN   \n",
       "7   7  -14  -15  -15  -15  -16  -17  -18  -18  -20   ...       NaN     NaN   \n",
       "8   8  248  298  330  358  378  387  391  393  396   ...       NaN     NaN   \n",
       "9   9 -454 -443 -426 -411 -396 -374 -346 -321 -298   ...       NaN     NaN   \n",
       "\n",
       "   x16374  x16375  x16376  x16377  x16378  x16379  x16380  x16381  \n",
       "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "2   -46.0   -46.0   -46.0   -46.0   -47.0   -48.0   -49.0   -49.0  \n",
       "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "5     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "6     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "7     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "8     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "9     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[10 rows x 16383 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(train_input['id'].dtype)\n",
    "train_input = train_input.sort_values(by=['id'])\n",
    "train_input = train_input.drop(columns=['id'])\n",
    "train_output = train_output.sort_values(by=['id'])\n",
    "train_output = train_output.drop(columns=['id'])\n",
    "test_input = test_input.sort_values(by=['id'])\n",
    "test_input = test_input.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing whats the total number of NaNs per feature\n",
    "((train_input.isna().sum() / num_of_features * 100) > 20).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16382"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input.loc[0])\n",
    "# train_input = train_input.loc[:9000]\n",
    "# test_input = train_input.loc[:9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_size = 9000\n",
    "new_data = np.zeros((len(train_input), max_size))\n",
    "for i in range(0, len(train_input)):\n",
    "    row = train_input.loc[i]\n",
    "    index = train_input.columns.get_loc(row.last_valid_index())\n",
    "    dummy = np.array(row[:index+1])\n",
    "    if (max_size - len(dummy)) <= 0:\n",
    "        new_data[i, :] = dummy[0:max_size]\n",
    "    else:\n",
    "        b = dummy[0:(max_size - len(dummy))]\n",
    "        goal = np.hstack((dummy, b))\n",
    "        while len(goal) != max_size:\n",
    "            b = dummy[0:(max_size - len(goal))]\n",
    "            goal = np.hstack((goal, b))\n",
    "        new_data[i, :] = goal\n",
    "train_input = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.zeros((len(test_input), max_size))\n",
    "for i in range(0, len(test_input)):\n",
    "    row = test_input.loc[i]\n",
    "    index = test_input.columns.get_loc(row.last_valid_index())\n",
    "    dummy = np.array(row[:index+1])\n",
    "    if (max_size - len(dummy)) <= 0:\n",
    "        new_data[i, :] = dummy[0:max_size]\n",
    "    else:\n",
    "        b = dummy[0:(max_size - len(dummy))]\n",
    "        goal = np.hstack((dummy, b))\n",
    "        while len(goal) != max_size:\n",
    "            b = dummy[0:(max_size - len(goal))]\n",
    "            goal = np.hstack((goal, b))\n",
    "        new_data[i, :] = goal\n",
    "test_input = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 9000)\n",
      "(10, 9000)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(((pd.DataFrame(test_input).isna().sum() / num_of_features * 100) > 0).sum())\n",
    "print(((pd.DataFrame(train_input).isna().sum() / num_of_features * 100) > 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = np.array(train_input)\n",
    "test_input = np.array(test_input)\n",
    "train_output = np.array(train_output)\n",
    "check = 6000 #dont know\n",
    "big = check\n",
    "Label_set = np.zeros((len(train_input), 4)) #Creating of one-hot encodings representation\n",
    "for i in range(len(train_output)):\n",
    "    dummy = np.zeros((4))\n",
    "    dummy[int(train_output[i])] = 1\n",
    "    Label_set[i, :] = dummy\n",
    "train_input = (train_input - train_input.mean())/(train_input.std()) #Some normalization here\n",
    "train_input = np.expand_dims(train_input, axis=2) #For Keras's data input size\n",
    "values = [i for i in range(len(train_output))]\n",
    "permutations = np.random.permutation(values) #shuffling\n",
    "train_input = train_input[permutations, :]\n",
    "Label_set = Label_set[permutations, :]\n",
    "Y_train = Label_set\n",
    "test_input = (test_input - test_input.mean())/(test_input.std()) #Some normalization here\n",
    "test_input = np.expand_dims(test_input, axis=2) #For Keras's data input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 9000, 1)\n",
      "(10, 9000, 1)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "print(test_input.shape)\n",
    "print(train_input.shape)\n",
    "print(Label_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten, LSTM, Conv1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "output_dir = './output'\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_input, Label_set, test_size=0.2)\n",
    "def change(x):  #From boolean arrays to decimal arrays\n",
    "    answer = np.zeros((np.shape(x)[0]))\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        max_value = max(x[i, :])\n",
    "        max_index = list(x[i, :]).index(max_value)\n",
    "        answer[i] = max_index\n",
    "    return answer.astype(np.int)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, 5, activation='relu', input_shape=(9000, 1)))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\t\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_dir, 'Best_model.h5'), monitor='val_acc', verbose=1, save_best_only=True)\n",
    "    hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=2, epochs=200, verbose=2, shuffle=True, callbacks=[checkpointer])\n",
    "    val_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_acc']\n",
    "    acc = hist.history['acc']\n",
    "#     pd.DataFrame(acc).to_csv(path_or_buf='Conv_models/Train_acc.csv')\n",
    "#     pd.DataFrame(val_acc).to_csv(path_or_buf='Conv_models/Val_acc.csv')\n",
    "#     pd.DataFrame(val_loss).to_csv(path_or_buf='Conv_models/Val_loss.csv')\n",
    "#     pd.DataFrame(hist.history).to_csv(path_or_buf='Conv_models/History.csv')\n",
    "    predictions = model.predict(X_val)\n",
    "    score = accuracy_score(change(Y_val), change(predictions))\n",
    "    print('Last epoch\\'s validation score is ', score)\n",
    "    df = pd.DataFrame(change(predictions))\n",
    "#     df.to_csv(path_or_buf='Conv_models/Preds_' + str(format(score, '.4f')) + '.csv', index=None, header=None)\n",
    "#     pd.DataFrame(confusion_matrix(change(Y_val), change(predictions))).to_csv(path_or_buf='Conv_models/Result_Conf' + str(format(score, '.4f')) + '.csv', index=None, header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8 samples, validate on 2 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 1.2792 - acc: 0.6250 - val_loss: 1.3186 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50000, saving model to ./output/Best_model.h5\n",
      "Epoch 2/200\n",
      " - 0s - loss: 1.1592 - acc: 0.7500 - val_loss: 1.2842 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.50000\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6830 - acc: 0.6250 - val_loss: 1.3228 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.50000\n",
      "Epoch 4/200\n",
      " - 0s - loss: 1.3215 - acc: 0.6250 - val_loss: 1.3112 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50000\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.7184 - acc: 0.5000 - val_loss: 1.3121 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50000\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.9572 - acc: 0.2500 - val_loss: 1.3293 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50000\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.7860 - acc: 0.7500 - val_loss: 1.3373 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.50000\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.9595 - acc: 0.6250 - val_loss: 1.4287 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.50000\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.8019 - acc: 0.6250 - val_loss: 1.4328 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.50000\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.7267 - acc: 0.3750 - val_loss: 1.4258 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.50000\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.7358 - acc: 0.6250 - val_loss: 1.4593 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.50000\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.8827 - acc: 0.2500 - val_loss: 1.4849 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.50000\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.7392 - acc: 0.6250 - val_loss: 1.4542 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.50000\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.7416 - acc: 0.3750 - val_loss: 1.4502 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.50000\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.7471 - acc: 0.5000 - val_loss: 1.4433 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.50000\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.6373 - acc: 0.5000 - val_loss: 1.5153 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.50000\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.8758 - acc: 0.6250 - val_loss: 1.5326 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.50000\n",
      "Epoch 18/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a9b8d43e16e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-ec44b583e1b7>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/venvs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Documents/venvs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/venvs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/venvs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/venvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biosppy.signals import ecg\n",
    "inputs = 60 \n",
    "new_data = np.zeros((len(train_input), inputs))\n",
    "train_input_n = np.array(train_input)\n",
    "for i in range(0, len(train_input_n)):\n",
    "#     row = train_input.loc[i]\n",
    "#     index = train_input.columns.get_loc(row.last_valid_index())\n",
    "#     dummy = np.array(row[:index+1])\n",
    "    out = ecg.christov_segmenter(signal=train_input_n[i, :], sampling_rate=300.)\n",
    "    A = np.hstack((0, out[0][:len(out[0]) - 1]))\n",
    "    B = out[0]\n",
    "    dummy = np.lib.pad(B - A, (0, inputs - len(B)), 'constant', constant_values=(0))\n",
    "    new_data[i, :] = dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_input.loc[0][:8851].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.loc[256][: 8602].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_per_feature = train_input.mean()\n",
    "train_input_fill1 = train_input.fillna(average_per_feature)\n",
    "train_input_fill1.loc[256][:].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biosppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biosppy.signals import ecg\n",
    "out = ecg.ecg(signal=train_input, sampling_rate=1000., show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biosppy.signals.ecg.extract_heartbeats(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fill1 = train_input.interpolate(method='linear')\n",
    "train_input_fill1.loc[8][:].plot()\n",
    "plt.show()\n",
    "train_input.loc[8][:].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE second filling NANs seems like a good one, which is linear interpolation\n",
    "train_input = train_input.interpolate(method='linear', axis=1)\n",
    "test_input = test_input.interpolate(method='linear', axis=1)\n",
    "train_output = train_output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "((train_input.isna().sum() / num_of_features * 100) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_output_shape = train_output.shape\n",
    "original_train_input_shape = train_input.shape\n",
    "original_test_input_shape = test_input.shape\n",
    "print(train_output.shape)\n",
    "print(train_input.shape)\n",
    "print(test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZING PARAMS\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_input, train_output, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALING\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 18154 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 4 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 18154, 1])\n",
    "        conv1 = tf.layers.conv1d(x, 128, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling1d(conv1, 2, 2)\n",
    "        conv1 = tf.layers.dropout(conv1, rate=dropout, training=is_training)\n",
    "\n",
    "#         # Convolution Layer with 64 filters and a kernel size of 3\n",
    "#         conv2 = tf.layers.conv1d(conv1, 128, 5, activation=tf.nn.relu)\n",
    "#         # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "#         conv2 = tf.layers.max_pooling1d(conv2, 2, 2)\n",
    "#         conv2 = tf.layers.dropout(conv2, rate=dropout, training=is_training)\n",
    "\n",
    "#         # Convolution Layer with 64 filters and a kernel size of 3\n",
    "#         conv3 = tf.layers.conv1d(conv2, 128, 5, activation=tf.nn.relu)\n",
    "#         # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "#         conv3 = tf.layers.max_pooling1d(conv3, 2, 2)\n",
    "#         conv3 = tf.layers.dropout(conv3, rate=dropout, training=is_training)\n",
    "        \n",
    "        \n",
    "#         # Convolution Layer with 64 filters and a kernel size of 3\n",
    "#         conv4 = tf.layers.conv1d(conv3, 128, 5, activation=tf.nn.relu)\n",
    "#         # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "#         conv4 = tf.layers.max_pooling1d(conv4, 2, 2)\n",
    "#         conv4 = tf.layers.dropout(conv4, rate=dropout, training=is_training)\n",
    "        \n",
    "#         # Convolution Layer with 64 filters and a kernel size of 3\n",
    "#         conv4 = tf.layers.conv1d(conv4, 128, 5, activation=tf.nn.relu)\n",
    "#         # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "#         conv5 = tf.layers.max_pooling1d(conv5, 2, 2)\n",
    "#         conv4 = tf.layers.dropout(conv4, rate=dropout, training=is_training)\n",
    "        \n",
    "#         # Convolution Layer with 64 filters and a kernel size of 3\n",
    "#         conv6 = tf.layers.conv1d(conv5, 128, 5, activation=tf.nn.relu)\n",
    "#         # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "#         conv6 = tf.layers.max_pooling1d(conv6, 2, 2)\n",
    "        \n",
    "        \n",
    "#         conv6 = tf.layers.average_pooling1d(conv6, 2, 2)\n",
    "        \n",
    "#         conv6 = tf.layers.dropout(conv6, rate=dropout, training=is_training)\n",
    "#         # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv1)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 256)\n",
    "#         # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "        \n",
    "        fc2 = tf.layers.dense(fc1, 128)\n",
    "        fc2 = tf.layers.dropout(fc2, rate=dropout, training=is_training)\n",
    "        \n",
    "        fc3 = tf.layers.dense(fc2, 64)\n",
    "        fc3 = tf.layers.dropout(fc3, rate=dropout, training=is_training)\n",
    "\n",
    "#         Output layer, class prediction\n",
    "        out = tf.layers.dense(fc3, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    logging_hook = tf.train.LoggingTensorHook({\"loss\" : loss_op, \n",
    "    \"accuracy\" : acc_op}, every_n_iter=1)\n",
    "    print(acc_op)\n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op},\n",
    "      training_hooks= [logging_hook])\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Estimator\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': np.array(X_train)}, y=np.array(y_train),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': np.array(X_test)}, shuffle=False)\n",
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "preds = np.array(preds)\n",
    "test_acc = f1_score(preds, np.array(y_test), average='micro')\n",
    "preds\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ON TESTING DATASET RESERVED\n",
    "#predict now\n",
    "test_input = scaler.transform(test_input)\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': np.array(test_input)}, shuffle=False)\n",
    "# Use the model to predict the images class\n",
    "test_input_pred = list(model.predict(input_fn))\n",
    "# test_input_pred = test_input_pred.squeeze()\n",
    "# test_input_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the output\n",
    "#interpolate\n",
    "#Scale data\n",
    "predicted_output = {'y': test_input_pred}\n",
    "predicted_output_df = pd.DataFrame(data=predicted_output)\n",
    "predicted_output_df.to_csv(\"task3/y_test.csv\", index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_input_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_input_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.interpolate()\n",
    "((train_input.isna().sum() / num_of_features * 100) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_input:\n",
    "    train_input[col] = pd.to_numeric(train_input[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.interpolate(axis=1)\n",
    "((train_input.isna().sum() / num_of_features * 100) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(C=0.5,kernel='rbf', gamma='scale', probability=True)\n",
    "model.fit(train_input, np.array(train_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = {'y': test_input_pred}\n",
    "predicted_output_df = pd.DataFrame(data=predicted_output)\n",
    "predicted_output_df.to_csv(\"task3/y_test.csv\", index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
